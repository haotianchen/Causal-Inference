---
title: '200C Section: Selection on Observables'
author: 'Haotian (Barney) Chen'
date: "April 26, 2024 + May 3, 2024 + May 10, 2024"  
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE, fig.align = 'center')
```
\newcommand{\y}{\mathbf{y}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\independent}{\perp\!\!\!\perp}

I develop this document based on materials from the previous TAs for PS 200C, Doeun Kim and Soonhong Cho. 

> The First Commandment of Causal Inference: A result is causal if you can support the assumptions that make it causal, not because you performed some procedure.


## Selection on Observables

***
 - Recall that random assignment guarantees **ignorability**: $\E[Y_{1i}|D_i=1] = \E[Y_{1i}|D_i=0]$ and $\E[Y_{0i}|D_i=1] = \E[Y_{0i}|D_i=0]$. In observational studies, however, it is not always met. 
  
 - Goal of SOO: once we condition on some **pre-treatment** observable covariates, treatment is as-if random assignment. We are "finding experiments" in the data. We will discuss estimation strategies including subclassification, matching, weighting, propensity score methods, and regression. 

 -  We invoke the **<span style="color:red;">conditional ignorability</span>** (selection on observables; conditionally as-if random) assumption: $$ \{ Y_{0i}, Y_{1i} \} \independent D_i|X_i=x \; \text{ for any } \; x \in \chi,$$ which means that among units with same values of covariates $X_i$, $D_i$ is "as-if" randomly assigned. In other words, the conditional distribution of potential outcomes is identical across levels of the treatment once we condition on the covariates $X$ (recall stratified/block randomized experiment, or "conditional experiment": "we effectively run a separate experiment within each stratum of $X$", but now allowing for continuous $X$s). **Hint**: this relates to Pset 3, Q1. 
 
 - Similarly, we have weaker versions of the conditional ignorability: $\{Y_{1i} \} \independent D_i|X_i=x \; \text{ for any } \; x \in \chi$ or $\{ Y_{0i} \} \independent D_i|X_i=x \; \text{ for any } \; x \in \chi$. 

 - We also need the **<span style="color:red;">common support (positivity)</span>** assumption: $$0<\Pr(D_i=1|X_i=x)<1 \; \text{ for any } \; x \in \chi,$$ which means that "with any value of $X_i$, unit could have received either treatment or  control."
 
 - Identification of $ATE$ under conditional ignorability:
 $$
 \begin{align*}
 \tau_{ATE} &= \E[Y_{1i} - Y_{0i}] \\
 &= \E_X[\E[Y_{1i} - Y_{0i} | X_i]]    &\text{(law of iterated expectation)} \\
 &= \int \underbrace{\E[Y_{1i} - Y_{0i}|X_i=x]}_{\tau(x):\text{ATE among subgroup }X_i=x} P(x)dx  &\text{(definition of } \E) \\
 &= \int (\E[Y_{1i}|X_i=x] - \E[Y_{0i}|X_i=x] ) P(x)dx  &\text{(linearity of }\E) \\
 &= \int (\E[Y_{1i}|X_i=x, \color{black}{D_i=1}] - \E[Y_{0i}|X_i=x, \color{black}{D_i=0}]) P(x)dx  &\text{(conditional ignorability + common support)} \\
 &= \color{red}{\int (\E[Y_i|X_i=x, D_i=1] - \E[Y_i|X_i=x, D_i=0]) P(x)dx}  &\text{(consistency, SUTVA)}\\
 &= \E_X [ \overbrace{ \underbrace{\E[Y_{i}|X_i=x, D_i=1]}_{\text{weighted mean of treated w/ }X_i=x} - \underbrace{\E[Y_i|X_i=x, D_i=0]}_{\text{weighted mean of control w/ }X_i=x} }^{\text{Conditional Expectation Function; let's call it } \hat{\tau}(x)} ]\\
 &= \E_X [\hat{\tau}(x)].
 \end{align*}
 $$
 Recall that regression can be used as just an estimator for conditional expectation function ("agnostic" view). Thus, ATE can be identified with regression function, $$\hat{\tau}(x) = \E[Y_i|D_i=1, X_i=x] - \E[Y_i|D_i=0, X_i=x],$$ where $\hat{\beta}_{D}$ would capture the average treatment effect.
 
 - The identification result for ATT ($\E[Y_{1i}-Y_{0i}|D_i=1]$) is similar, but replace $f(x)$ with $f(x|D_i=1)$: for ATT, we now take expectation with respect to the distribution of $X_i$ given $D_i=1$ (instead of the marginal distribution of $X$). **Hint**: this relates to Pset 3, Q1. 
  $$
 \begin{align*}
 \tau_{ATT} &= \E[Y_{1i} - Y_{0i} | D_i=1] \\
 &= \E_X[\E[Y_{1i} - Y_{0i} | X_i, D_i=1]]    &\text{(law of iterated expectation)} \\
 &= \int \underbrace{\E[Y_{1i} - Y_{0i}|X_i=x, D_i=1]}_{\tau(x):\text{ATT among subgroup }X_i=x} P(x|D_i=1)dx  &\text{(definition of } \E) \\
 &= \int (\E[Y_{1i}|X_i=x, D_i=1] - \E[Y_{0i}|X_i=x, D_i=1] ) P(x|D_i=1)dx  &\text{(linearity of }\E) \\
 &= \int (\E[Y_{1i}|X_i=x, \color{black}{D_i=1}] - \E[Y_{0i}|X_i=x, \color{black}{D_i=0}]) P(x|D_i=1)dx  &\text{(weaker conditional ignorability)} \\
 &= \color{red}{\int (\E[Y_i|X_i=x, D_i=1] - \E[Y_i|X_i=x, D_i=0]) P(x|D_i=1)dx}  &\text{(consistency, SUTVA)}\\
 \end{align*}
 $$
Note that we only need the weaker version of the conditional ignorability here. 

 <br>

## Subclassification

***

- In subclassification, we 1) partition the data by covariates X, 2) calculate the difference in means of $Y_i$ between the treated and untreated within each stratum, and 3) calculate the weighted average of (2), with weights equal to the proportions of units in the strata.

- We can write down the subclassification estimators: 
$$
\hat \tau_{ATE} = \sum_{j=1}^{M}\{\overline Y_{1j} - \overline Y_{0j}\}\frac{n_j}{n}
$$
$$
\hat \tau_{ATT} = \sum_{j=1}^{M}\{\overline Y_{1j} - \overline Y_{0j}\}\frac{n_1j}{n_1}
$$
where $M$ is the number of strata. 

<br>

## Matching

***

### Matching as an estimation strategy (it is NOT an identification strategy)
 
 - Subclassification breaks when you have many $X$'s.

 - It's worth repeating: identification precedes estimation. Let’s say that we know that ignorability holds conditional on observed covariates $X_i$. That is, we can get reasonable answer with our data (we can identify) for our target quantity (estimand). Then how can we "estimate" the estimand? Or, what is the best way to obtain an estimate with "adjusting for" $X_i$? Some broad approaches, which overlap in part but also have fundamental differences, include **matching**, **weighting**, and **regression adjustment**.
 
 - Ignorability justifies a causal claim we make, matching does not. Matching doesn't make identification assumptions more plausible; theoretical concerns and domain knowledge does. However, matching has two benefits: 1) it can simplify the analysis of causal effects (no complicated statistical modeling required, if covariates are well-balanced), and 2) it can reduce dependence of estimates on parametric models.

 - Then why use matching? Given the proper set of observed confounders (i.e., assuming SOO), matching is an **estimation** strategy that helps **<span style="color:red;">reduce model dependence</span>**.

 - Idea: "balance" the distribution of covariates (X) in the treated and control groups. Matching is a non-parametric estimator of the counterfactual world. 
 
 - In practice we have to make many calls: which covariates, what distance metric, with or without replacement, whether one-on-one, etc.
 
 - Individual treatment effect for **treated unit** $i$ is $$\tau_i = \underbrace{Y_i}_{\text{observed}} - \underbrace{Y_{0i}}_{\text{unobserved}}$$
 
 - Estimand: ATT, $\E[Y_{1i} - Y_{0i}|X_i, \color{red}{D_i=1}]$ (for each treated units, match them with "similar" control units, and discard the rest).
 
 - Estimator: 
    + Closest match: $$\hat{\tau}_{ATT}  = \frac{1}{N_1} \sum_{D_i=1} [Y_i - Y_{j(i)}] $$ where $Y_{j(i)}$ is the imputed counterfactual value of $i$ by matching.
    + M closest matches: $$\hat{\tau}_{ATT}  = \frac{1}{N_1} \sum_{D_i=1} [Y_i - \frac{1}{M} \sum_{j \in M_i}Y_{jm(i)}] $$ where $Y_{j(i)}$ is the imputed counterfactual value of $i$ by matching, $M_i$ is the "matched set" for treated unit $i$.
    + Generally: $$\hat{\tau}_{ATT}  = \frac{1}{N_1} \sum_{D_i=1} [Y_i - \omega_{ij} \sum_{j=1}Y_{j(i)}] $$ where $Y_{j(i)}$ is the imputed counterfactual value of $i$ by matching, $\omega_{ij}$ depends on the matching procedure (e.g., difference in propensity score, euclidean distance, StataD, Mahalanobis distance, GeneticD).

<br>

Here's an animation of how it works intuitively:
<br>

<p align="center">
![ ](matching.gif)
</p>

(Credit: Apoorva Lal and Nick Huntington-Klein)

<br>

#### Advantages: 

 - **Ho, Imai, King, Stuart, Matching as Nonparametric Preprocessing for Reducing Model Dependence in Parametric Causal Inference** ([2007](https://www.cambridge.org/core/journals/political-analysis/article/matching-as-nonparametric-preprocessing-for-reducing-model-dependence-in-parametric-causal-inference/4D7E6D07C9727F5A604E5C9FCCA2DD21)), Fig.1:
 
```{r, echo=FALSE, fig.align='center', fig.width = 10, fig.height=5}
library(MatchIt)
library(scales)
load("Ho_et_al_figure_1.Rdata")

## Ho, Imai, King, Stuart (2007): fig.1, Political Analysis
lm.all1 <- lm(y ~ t+x, data=dta)
lm.all2 <- lm(y ~ t+x+I(x^2), data=dta)
temp <- matchit(t ~ x, data=dta)
matched <- match.data(temp)
lm.m1 <- lm(y ~ t+x, data=matched)
lm.m2 <- lm(y ~ t+x+I(x^2), data=matched)

plot.pts <- seq(from=min(dta$x),to=max(dta$x),by=0.1)
plot.pts2 <- seq(from=5, to=24, by=0.1)

par(mar=c(2, 2, 2, 2) + 0.1, cex.lab=0.7, cex.axis=0.5,
    mgp=c(1,0.5,0), cex.main=0.8, cex=1, mfrow=c(1,2), bg="white")
plot(dta$x[dta$t==1],dta$y[dta$t==1],pch="T", col="red",
     xlim=range(dta$x), ylim = range(dta$y),
     xlab="X", ylab = "Y", cex=0.8, main="Before Matching")
points(dta$x[dta$t==0],dta$y[dta$t==0],pch="C", cex=0.8, col="blue")
abline(lm.all1$coef[1] + lm.all1$coef[2], lm.all1$coef[3], lty=1, col="red", lwd=1.5)
abline(lm.all1$coef[1], lm.all1$coef[3], lty=1, col="blue", lwd=1.5)
lines(plot.pts, lm.all2$coef[1] + lm.all2$coef[2] +
     lm.all2$coef[3]*plot.pts + lm.all2$coef[4]*plot.pts^2,
      lty=2, col="red", lwd=1.5)
lines(plot.pts, lm.all2$coef[1] + lm.all2$coef[3]*plot.pts +
      lm.all2$coef[4]*plot.pts^2, lty=2, col="blue", lwd=1.5)
legend(5, 4.75, lty=c(1, 1, 2, 2), col=c(1, 8, 1, 8), lwd=1.5,
               legend=c("Linear Model, Treated Group", "Linear Model, Control Group",
                 "Quadratic Model, Treated Group", "Quadratic Model, Control Group"), cex=0.5)
plot(matched$x[matched$t==1], matched$y[matched$t==1],
     pch="T", xlab="X", ylab="Y", col="red", xlim=range(dta$x),
     ylim=range(dta$y), cex=0.8, main="After Matching")
points(matched$x[matched$t==0], matched$y[matched$t==0], pch="C", cex=0.8, col="blue")
points(dta$x[temp$weights==0 & dta$t==0],
       dta$y[temp$weights==0 & dta$t==0],
       pch="C", col=alpha("blue", 0.4), cex=0.8)
lines(plot.pts2, lm.m1$coef[1] + lm.m1$coef[2] + lm.m1$coef[3]*plot.pts2, lty=1, col="red", lwd=1.5)
lines(plot.pts2, lm.m1$coef[1] + lm.m1$coef[3]*plot.pts2, lty=1, col="blue", lwd=1.5)
lines(plot.pts2, lm.m2$coef[1] + lm.m2$coef[2] +
      lm.m2$coef[3]*plot.pts2 + lm.m2$coef[4]*plot.pts2^2,
      lty=2, col="red", lwd=1.5)
lines(plot.pts2, lm.m2$coef[1] + lm.m2$coef[3]*plot.pts2 +
      lm.m2$coef[4]*plot.pts2^2, lty=2, col="blue", lwd=1.5)
legend(5, 4.75, lty=c(1, 1, 2, 2), col=c(1, 8, 1, 8), lwd=1.5,
       legend=c("Linear Model, Treated Group", "Linear Model, Control Group",
         "Quadratic Model, Treated Group", "Quadratic Model, Control Group"), cex=0.5)
```

 - **<span style="color:red;">Reducing model dependence</span>** and **<span style="color:red;">ensuring common support</span>**: without matching, the model we select for the relationship between $X_i$ and $Y_i$ will affect the estimates---we get different model fit for linear model and quadratic model with $X_i^2$ term, and moreover the effect estimate has opposite sign (positive to negative). It's mainly due to control units that different from treated units in $X_i$, which would be "pruned" by matching. In other words, matching "balances" observed covariates across treatment groups, i.e., $P(X_i=x|D_i=1) = P(X_i=x|D_i=0, \text{matched})$.


#### Disadvantages: 

 - Bias arise with more continuous covariates.
 - Abadie-Imbens bias adjustment: $$\hat{\tau}_{ATT}  = \frac{1}{N_1} \sum_{D_i=1} [Y_i - Y_{j(i)} - (\hat \mu_0(X_i) - \hat \mu_0(X_{j(i)}))]$$ where $\mu_0(x) = \E[Y|X, D=0]$. In R, use `Match(Y, Tr, X, BiasAdjust = TRUE)`

 <br>

### Matching Coding Example

Example: Blattman and Annan (2009). We want to investigate the impact of being abducted by the LRA (the treatment; `abd`) on education (outcome variable; `educ`). **Hint**: this whole section relates to Pset 3, Q2 and Q3. 


```{r message=FALSE}
library(foreign) # Loading normally unsupported file formats
library(Matching) # Finding matches
library(ebal) # Entropy balancing weights
library(cobalt) # Easy love plots

dat <- read.csv("BlattmanAnnan.csv")

# Let's take a look at the top few rows of the data frame:
head(dat)
# abd is the treatment
```

#### Step 1: Check the covariate balance 

First, before reporting actual balance statistics let's take a look at what seems to predict treatment status. This implies a regression where the dependent variable is treatment status and the predictors are every other covariates we can fit.

What are some of the methods we can predict balance? We could try a multiple regression setting:

```{r}
# We keep covariates to use going forward
covariates <- c("age", "fthr_ed", "mthr_ed", "hh_size96", "orphan96", "C_ach",
                "C_akw", "C_ata", "C_kma", "C_oro", "C_pad", "C_paj", "C_pal")

# We can specify a formula using `paste` function for use later
balance_formula <- as.formula(paste("abd ~ ", paste(covariates, collapse=" + ")))
balance_formula

summary(lm(balance_formula, data=dat))
```

We could plot the PDF of each covariate between treatment and control:

```{r, fig.width=6, fig.height=4}
library(tidyverse)

dat %>% dplyr::select(abd, all_of(covariates)) %>%
  mutate(abd = as_factor(ifelse(abd==1, "Treaed", "Control"))) %>% 
  reshape2::melt(all_of(covariates), id.vars = "abd") %>% #make it "long format" for plotting
  ggplot(aes(x = value)) +
  geom_density(aes(col = as_factor(abd)), alpha = 0.4) +
  facet_wrap(~ variable, scales = "free") +
  labs(title=paste0("PDF of Covariates by Treatment Status"), 
       y="Density",col = "Treatment") +
  labs(col = "Treatment") +
  theme_minimal() +
  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank())
```

We can also use `MatchBalance` function in `Matching` package:

```{r}
library(Matching)

# First argument: A formula describing the balance we're checking. 
# The lefthand side is the dependent variable (in our case, treatment status). 
# The right hand side is all the variables you want to check balance on.

# nboots because Matching uses bootstrapping to get SEs on the KS test.
# How many boots? Default is 500, but make it way higher.

mb <- MatchBalance(balance_formula, data=dat, nboots=10000, print.level = 0) # set print.level = 0
```

Let's use `baltest.collect` from `ebal` to get the results nicely.

```{r}
library(ebal)
# baltest.collect takes three arguments: first, the Matching object.
# Second, which variables do you care about balance on.
# We use a vector of character strings to specify the variable names: 

variable_names = all.vars(balance_formula)[-1]
# Why do we do [-1]? The first variable is the treatment status, and we don't need "balance" on that.

# Third, do you want to see balance before any Matching attempt, or after?
# We want to see before, so we use after = FALSE
balance_test <- baltest.collect(mb,
                        var.names = covariates,
                        after = FALSE)

# balance_test includes a bunch of stuff we don't care about. Let's discard down:
balance_test <- balance_test[, c("mean.Tr", "mean.Co", "T pval", "KS pval")]

# Let's make a nice kable table:
library(knitr)
library(kableExtra) #for aesthetics
balance_test %>%
  kable(booktabs = T, digits=3, align = rep('c', 4),
        col.names = c("Treatment Mean", "Control Mean", "T p-value", "Ks p-value")) %>% 
  kable_styling(full_width = F, position = "center")
```

To sum up, we want to check covariate balance [$Pr(X|D=1) \approx Pr(X|D=0)$] first. The KS test is used to test whether two random variables are sampled from the same distribution. 


#### Step 2: Matching

Let's do some matching. As mentioned above, you need to make some calls: 1) With or without replacement? 2) How many matches? 3) Which matching algorithm? 

Here, let's try to extract the ATT --- so we'll use our real treatment observations, matched control observations, and discard the rest. We'll do a simple match; one match per treated unit. And we'll use replacement (so the same control unit can be picked as a match for more than one treated unit).

We also need to specify which variables we want to match on exactly. First we'll subset to the variables we care about, and then we'll take a peek at those variable to see whether they're likely to need exact matches:

```{r}
outcome <- dat$educ
treated <- dat$abd
X <- dat[, covariates]
str(X)
```

We don't have any variables that are obviously "continuous", but a few look to me like they can take many possible values: `age` and `hh_size96`. Let's just briefly confirm that:

```{r}
length(unique(X$age))
length(unique(X$hh_size96))
```

We've only got 741 observations so with 23 possible values of household size and 17 of age we probably don't want exact matching for those. The rest all look like they're binary or only take a few values, so for them we'll do exact matching.

```{r}
exact_matches <- ifelse(colnames(X) %in% c("age", "hh_size96"), FALSE, TRUE)
```

How does `ifelse` work? It takes three arguments and outputs a vector. The first argument is a statement to evaluate. We're going to map every single column name, and check for each of them are they "in" the vector `c("age", "hh_size96")`. If they are, use the value `FALSE` for that item in the vector. If they are not, use the value `TRUE` for that item in the vector.

Now let's do the matching.

```{r}
match_model_1 <- Match(Y = outcome,
                       Tr = treated,
                       X = X,
                       M = 1, # one-to-one matching
                       exact = exact_matches,
                       estimand = "ATT")

# run ?Match to check all the arguments
```

What are the arguments here? Well, it's obvious you need `Y` to estimate a treatment effect. It's obvious you need `Tr` so it knows which units are treated and which are controlled. `X` is the covariates to match on. `M` tells you how many matches you want for each treatment unit. `exact` says, for each covariate, if you want an exact match or not. `estimand` tells `Matching` your estimand: in our case we want to keep the real treated units and find matching control units.

We can also easily do bias adjustment:

```{r}
match_model_2 <- Match(Y = outcome,
                       Tr = treated,
                       X = X,
                       M = 1, # one-to-one matching
                       exact = exact_matches,
                       estimand = "ATT",
                       BiasAdjust = TRUE)
```

Bias Adjustment compensates for the fact that since we can't find exact matches, the effect size will be a little skewed (imagine there's a relationship between X and Y, but the Xs of treated units are all slightly higher than the Xs of the control units they are matched with -- or vice versa -- wouldn't this bias our effect estimate?)

Let's now check what `Matching` actually gives us:


```{r}
ls(match_model_1)
```

This tells us what is contained in the `Match` object; We see a few we might like to check out:

```{r}
match_model_1$est
match_model_1$se # SEs from Abadie-Imbens
```

And with the bias-adjusted estimates:
```{r}
match_model_2$est
match_model_2$se
```

Compare to "naive" estimate:
```{r}
summary(lm(educ ~ abd, data = dat))
```

#### Step 3: Balance After Matching
Normally, before you even look at results you'd examine balance post-matching. What we're trying to do here is make sure that the Matching process actually resulted in good balance. And we can include variables that we didn't match on, if we want.

Note, you can check balance on things that weren't matched on to make sure they got balanced too. 

```{r}
match_balance_after <- MatchBalance(match.out = match_model_1,
                                    formul = balance_formula,
                                    data = dat,
                                    print.level = 0,
                                    nboots = 10000)

balance_test_after <- baltest.collect(match_balance_after,
                                      var.names = covariates,
                                      after = TRUE)[, c("mean.Tr", "mean.Co",
                                                        "T pval", "KS pval")]

balance_test_after %>%
  kable(booktabs = T, digits=3, align = rep('c', 4),
        col.names = c("Treatment Mean", "Control Mean", "T p-value", "Ks p-value")) %>% 
  kable_styling(full_width = F, position = "center")
```

If the balance does not look good, repeat Step 1-3 again.  

<br>


## Propensity Score Methods

***

### Motivation

The curse of dimensionality makes it difficult to work in a situation where there are many (pre-treatment) covariates to condition on.
 
 - Let's see what's the problem with a simple simulation: 1) create a dataset with 500 observations, in which a treatment $D_i$ is allocated so that the first 250 units in the data are treated and the rest are control, 2) draw each of the 20 covariates independently from the multinomial distribution with 4 categories, with probabilities $p = (0.1,0.2,0.3,0.4)$ for the treatment group and $p = (0.25,0.25,0.25,0.25)$ for the control group, then 3) match each treated unit to an untreated unit with exactly the same first covariate $x_{i1}$, with replacement, while storing the proportion of treated units matched. We'll repeat this with increasing dimensionality up to 20, and also with increasing the size of control units from 400 to 2,000 to 10,000.

```{r, echo=FALSE}
library(tidyverse)
library(Matching)
## dataset with 500 obs and 20 covariates
n<-500; n_t<-100; n_c<-n-n_t; p<-20 #100 treated and 400 control
X <- rbind(replicate(p, sample(1:4, size=n_t, replace=TRUE, #treat
                               prob=c(.1, .2, .3, .4))),
           replicate(p, sample(1:4, size=n_c, replace=TRUE))) #control (equal probabilities)
X <- cbind(X, treat=rep(c(1,0), c(n_t, n_c))) #treatment indicator

## How would the plot change with increasing control units, with fixing treat units?
X_2 <- rbind(X, cbind(replicate(p, sample(1:4, size=2000*20-500,
                                          replace=TRUE)), rep(0, 2000)))
X_3 <- rbind(X, cbind(replicate(p, sample(1:4, size=10000*20-500,
                                          replace=TRUE)), rep(0, 10000)))

## make a helper function to get proportion of exact matches
prop_matched <- function(data){
  match_res <- Match(Tr = data[, "treat"], X = data[, 1:(ncol(data)-1)],
                     M = 1, exact = TRUE, replace = TRUE, estimand = "ATT")
  #if no exact match we get FALSE, so assign 0 in that case
  matched <- ifelse(is.logical(match_res)==TRUE, 0, match_res$nobs - match_res$ndrops)
  return(matched/nrow(data))
}

plot_prop_matched <- function(X){
  map_dbl(1:(ncol(X)-1), #without treat indicator column
                            function(d){ prop_matched(X[, c(1:d, 21)]) })
}

plot_df <- tibble(dim=1:20, x1 = plot_prop_matched(X),
                  x2 = plot_prop_matched(X_2), 
                  x3 = plot_prop_matched(X_3)) %>% 
  gather(size, prop, -dim)

ggplot(plot_df, aes(x=dim, y=prop, col=size)) +
  geom_line() +
  labs(title="Curse of Dimensionality in Matching, with Varying Size",
       x="Dimensions (# of variables)", y="Proportion of Exact Matches",
       col="Control Group Size") +
  scale_color_hue(labels=c("400", "2,000", "10,000")) +
  theme_bw() + theme(legend.position=c(.8,.85))
```

- It seems that the “curse” is really a curse for matching. This plot shows the performance of exact matching does not get better that much even with many more control units to be matched on, and the improvement in performance drops quickly. It's because the comparability of matches is getting worse as more covariates are matched on (hard to find "close" pairs). We might want to balance as many observed variables as possible between treatment conditions, but matching will perform poorly with more covariates.

### Estimating the Propensity Score

 - **Propensity Score**: the probability of treatment conditional on the covariates, or $\pi_i = Pr(D_i=1|X_i)$. Using the propensity score, we can compare units that have a comparable probability of getting treated. In practice, we don’t know the true propensity score --- we usually run a parametric model (like `glm`) to estimate the propensity score. 
 
 - Assuming conditional ignorability and common support, we have $\{ Y_{0i}, Y_{1i} \} \independent D_i|\pi_i$, which is called the Balancing Property of the propensity score. Condition on $\pi_i$, $D_i$ does not depend upon $X_i$, thus also no longer depends on $\{ Y_{0i}, Y_{1i} \}$ (SOO). 
 
 - After estimating the propensity scores, we can 1) use subclassification or matching, or 2) weight by the inverse of the propensity score.  

### Propensity Score Coding Example
First, let's calculate the propensity scores using logistic regression. **Hint**: this section relates to Pset 3, Q3. 

```{r}
logit_fit <- glm(balance_formula, data = dat, family = binomial(link = logit))

# Extract these using predicted probabilities or with the direct fit object:
pi.out <- logit_fit$fitted.values

# Plot
ggplot(tibble(ps = pi.out, treatment = as_factor(ifelse(dat$abd==1, "Treated", "Control"))),
       aes(x=ps, col=treatment)) +
  geom_density() +
  labs(title="Distribution of propensity scores", y="Density", x="Propensity Score") +
  xlim(c(0,1)) +
  theme_minimal()
```

We can see that these distributions do not look quite the same. Your first intuition is likely to say "well, the treated look like they're more likely to take the treatment than the control units, isn't that normal?". But this is a sign of imbalance on its own. What we'd instead like is a sense that units, no matter what their propensity score, are in balanced proportions in the treated and control groups.

So, we're going to use our two methods. First, let's use `Match` to calculate the ATT with propensity score as the covariate:

```{r}
p_score_match <- Match(Y = outcome,
                       Tr = treated,
                       X = pi.out,
                       M = 1,
                       estimand = "ATT")
summary(p_score_match)
```

And checking the balance:
```{r}
p_score_match_balance <- MatchBalance(match.out = p_score_match,
                                      formul = balance_formula,
                                      data = dat,
                                      print.level = 0)

balance_table_ps_match <- baltest.collect(p_score_match_balance,
                                          var.names = covariates,
                                          after = TRUE)[,c("mean.Tr", "mean.Co", "T pval", "KS pval")]

balance_table_ps_match %>% 
  kable(booktabs = T, digits=3, align = rep('c', 4)) %>% 
  kable_styling(full_width = F, position = "center")
```

You can also use `cobalt` package to implement propensity score matching:

```{r}
m_out_ps <- matchit(balance_formula, data = dat, estimand = "ATT", 
                         ratio = 1, # 1-to-1 matching
                         replace = TRUE, # with replacement
                         #distance measure is propensity score estimated by logistic regression
                         distance = "glm", link = "logit", method = "nearest")

love.plot(m_out_ps, stat=c("m", "ks"))

#much better balance!
```


## Weighting

***

### Weighting with the Propensity Score

- Recall that the motivation for matching methods was to improve covariate balance between treatment groups. But matching methods have some limitations: (1) inefficient, since it may throw away data (non-matched control units), (2) ineffective, since it may not be able to balance covariates in expectation (if control pool is bad for matching). But we can use all the data we have with balancing covariates by reweighting the data in certain ways!

- The idea is to correct for non-random treatment assignment by weighting by the probability of selection into each group.

- **Inverse Probability-of-Treatment Weighting (IPW)**: IPW relies on the fact that we can estimate how likely a unit with some characteristics $\mathbf{X}$ would be to get treatment (so, "propensity" to get treated). Then the idea is simple: by reweighting units in a certain way, we will make a pseudo-dataset such that the covariates are balanced. Units that are “weird” (they got treatment when we would have expected them to get control, or vice versa) are weighted up, while units that got the expected result are weighted down – so the final weighted quantity will be closer to balanced. 

- Here is the simple IPW formula: $$IPW_i = \frac{1}{\Pr(D_i|X_i)} = \frac{1}{D_i \hat{\pi}_i + (1-D_i)(1-\hat{\pi}_i)}.$$ 


<br>

#### Reducing weight variation
- IPW estimators are consistent but biased in small samples, and this bias is severe when some weights are extremely large or small. 

- A simple workaround is trim units with extreme weights---discard units with too extreme weights (but trimming changes the estimand to a weird quantity that is causal yet difficult to interpret).

- Another option is to use "stabilized" IPW. which normalizes the weights according to the population distribution: $$SIPW_i = \frac{\Pr(D_i)}{\Pr(D_i|X_i)} = \frac{D_i \Pr(D_i=1) + (1-D_i)(1- \Pr(D_i))}{D_i \hat{\pi}_i + (1-D_i)(1-\hat{\pi}_i)}.$$

### Weighting Coding Example
```{r}
# recall from previous section we have: 
# logit_fit <- glm(balance_formula, data = dat, family = binomial(link = logit))
# pi.out <- logit_fit$fitted.values, which are the propensity scores 

plot(density(pi.out[dat$abd==1]), lwd=2, main="Distribution of pscores")
lines(density(pi.out[dat$abd==0]), lwd=2, lty=2)
legend("topleft", legend=c("treated","controls"), lty=c(1,2), lwd=2)

# with stabilized IPW weights
ps = pi.out
D = dat$abd
PrD = mean(D)
IPW = (D*PrD+(1-D)*(1-PrD))/(D*ps+(1-D)*(1-ps))

# check the propensity scores after weighting
plot(density(ps[D==1], weight=IPW[D==1]/sum(IPW[D==1])), lwd=2, main="Distribution of propensity scores: Weighted")
lines(density(ps[D==0], weight=IPW[D==0]/sum(IPW[D==0])),lwd=2, lty=2)
legend("topleft", legend=c("treated","controls"), lty=c(1,2), lwd=2)
```

Check omnibus balance:
```{r}
summary(lm(balance_formula, data = dat))
summary(lm(balance_formula, weight = IPW, data = dat)) 
```

Now use weights in regression:
```{r}
summary(lm(educ ~ abd, data = dat) )
summary(lm(educ ~ abd, weight = IPW, data = dat))
```

