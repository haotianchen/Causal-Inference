---
title: 'Pol Sci 200C Section: Probability, Linear Algebra, and OLS Refersher'
author: 'Haotian (Barney) Chen'
date: "April 4, 2025"
output:
  html_document: 
    math_method: katex
---
\newcommand{\y}{\mathbf{y}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\Cov}{\mathbb{Cov}}
\newcommand{\independent}{\perp\!\!\!\perp}

> This week's section will be a quick review of probability theory, linear algebra, and OLS. We will refer to several other documents, including the Math Prefresher (online), Basic Linear Algebra Review (pdf), OLS in Matrix Form (pdf), and Standard Errors in OLS (pdf). For the PDF files, they are easier to view if you download them from the Github repository. 

## Probability Theory
The material we will use for a quick review of probability theory is the **Math Prefresher for Political Scientists** by Harvard Government Dept, which you can access [here](https://iqss.github.io/prefresher/06_probability.html). 


### Simulation
For example, if we want to draw n sample size r times from normal distribution, calculate mean of each draw (i.e., sample mean), get an 80% confidence interval for the sample mean, and then compute coverage rate (proportion of estimates that fall within confidence interval):

```{r message=FALSE, warning=FALSE}
library(tidyverse)

sample_mean_ci <- function(n, r, pars){
  mu <- pars[1] # true mean is mu
  replicate(r, { # use replicate function to do r times
    samp <- rnorm(n, mean = pars[1], sd = sqrt(pars[2]))
    data.frame(mean = mean(samp),
               # using t-distribution confidence interval (CI)
               lower = mean(samp) - sd(samp)/sqrt(n)*qt(1 - 0.2/2, length(samp)-1),
               upper = mean(samp) + sd(samp)/sqrt(n)*qt(1 - 0.2/2, length(samp)-1))
  }, simplify = FALSE) %>% bind_rows() %>%
    rownames_to_column(var = "replicate") %>%
    mutate(cover = lower < mu & upper > mu) %>% # coverage of true mean
    mutate(coverage = mean(cover))
}

# store result: e.g., sample size 100, repeat 1000 times, Normal(1, 4)
true_mean <- 1
result <- sample_mean_ci(n = 20, r = 1000, pars = c(true_mean, 4))

# plot the first 100 of these intervals: for illustration purposes
ggplot(result, aes(x = lower, xend = upper, y = replicate, yend = replicate, col = cover)) +
  geom_segment() + 
  geom_vline(xintercept = true_mean) + # true mean here
  annotate(geom = "text", x = 0.5, y = 95, size = 4, 
           label = paste0("Coverage Rate:", round(result$coverage, 3)*100, "%")) +
  # plot first 100 rows by `limits` + adjust y-axis ticks by `breaks`
  scale_y_discrete(breaks = seq(25, 100, by = 25), limits = seq(1, 100, by = 1)) +
  labs(title = "80% Confidence Intervals for 100 random samples", x = "Sample Mean", y = "Replicate") +
  scale_colour_discrete(name = "Interval Covers Truth?") +
  theme_bw() + 
  theme(legend.position = "bottom")

# want to save batches of results to a table? search `kable`
```


## Linear Algebra
I prepare a short review of basic linear algebra, which you can access [here](https://github.com/haotianchen/Causal-Inference/blob/main/1-Math/Linear_Algebra_Review.pdf). You should also check the **Linear Algebra** section in the **Math Prefresher**. 


### Matrix in R
```{r}
# vector
v1 <- c(1, 2, 3, 4, 5)
v2 <- c(-1, -2, -3, -4, -5)
v1+v2

# matrix
A <- matrix(c(1, 2, 3, 4), nrow=2, byrow=FALSE)
B <- cbind(c(1, 0), c(1, 1))
A
B

# matrix addition
A+B

# scalar multiplication
2*A

# matrix multiplication
A %*% B

# transpose (A^T or A')
t(A) 

# trace
sum(diag(A))

# inverse
solve(A)
```


## OLS
The first document you need to check is the **OLS in Matrix Form**, which you can access [here](https://github.com/haotianchen/Causal-Inference/blob/main/1-Math/matrix_OLS_notes.pdf).  

After reading it, here is a summary of the key OLS assumptions and properties. Make sure you understand them well!

In the classical OLS regression model, we have the following assumptions:

+ (A.1) **Linearity**: $\Y = \X \mathbb{\beta} + \epsilon$
+ (A.2) **Strict Exogeneity**: $\E[\epsilon | \X] = 0$. The explanatory variables are exogenous (uncorrelated with the error term). It implies: 
  - (A.2.a) Weak Exogeneity: $\E[\X_i\epsilon_i] = 0$. The explanatory variable is orthogonal to the error term. 
  - (A.2.b) Unconditional Mean: $\E[\epsilon_i] = 0$. The error term has zero mean.
+ (A.3) **Full Rank**: $\text{rank}(\X)=k$ (we need $\X'\X$ to be invertible). A violation is *perfect multicollinearity* when some explanatory variables are linearly dependent.
+ (A.4) Spherical Errors: $\V[\epsilon | \X] = \E[\epsilon\epsilon'|\X] = \sigma^2 I_n$. It implies:
  - (A.4.a) **Homoskedasticity**: $\V[\epsilon_i | \X] = \sigma^2$. Error terms have constant variance. No heteroskedasticity. 
  - (A.4.b) **No Autocorrelation of Errors**: $\Cov[\epsilon_i, \epsilon_j | \X] = \E[\epsilon_i \epsilon_j | \X] = 0$ for $i \neq j$. No serial correlation.
+ (A.5) **Normality**: $\epsilon | \X \sim N(0, \sigma^2I_n)$. The error terms are normally distributed.

Under these assumptions, we can estimate the OLS estimator $\hat{\beta} = (\X'\X)^{-1}\X'Y$. Here are the properties of OLS Estimator: 

+ **Unbiasedness**: $\E[\hat{\beta} | \X] = \beta$. 
+ **Efficiency**: The Gaussâ€“Markov theorem states that the OLS estimator is BLUE (Best Linear Unbiased Estimator). 
+ **Consistency**: $\hat{\beta} \xrightarrow{p} \beta$ as $n \rightarrow \infty$.
+ **Asymptotically Normality**: $\hat{\beta} \sim N(\beta, \sigma^2 (\X'\X)^{-1})$.

### Heteroskedasticity-consistent SEs

Now, please carefully read this document on heteroskedasticity-consistent standard errors (I really mean it!), which you can access [here](https://github.com/haotianchen/Causal-Inference/blob/main/1-Math/se_ols.pdf). 

Recall from above that under the assumption of homoskedasticity ($\V[\epsilon_i | \X] = \sigma^2$), the variance of $\hat{\beta}$ is $\sigma^2 (\X'\X)^{-1}$. However, when the error terms have different variances, $\hat{\beta}$ is still unbiased but not efficient. The solution, which you often encounter in table footnotes in good empirical papers, is heteroskedasticity-consistent standard errors (people use the term "robust" SEs). 

The heteroskedasticity-consistent estimator gives us:

$$\V(\hat{\beta})_{\text{HC}} = (X'X)^{-1} X' \,\text{diag}(\hat{\epsilon}_1^2, \hat{\epsilon}_2^2, \dots, \hat{\epsilon}_n^2)\, X (X'X)^{-1}$$

where $(X'X)^{-1}$ is often called the "bread", and $X' \,\text{diag}(\hat{\epsilon}_1^2, \hat{\epsilon}_2^2, \dots, \hat{\epsilon}_n^2)\, X$ is called the "meat". The difference in the HC estimators (HC0, HC1, HC2, HC3) is how we choose to substitute the "meat" to allow for non-constant variance. 

In practice, it is recommended to use HC2. In R, you can use `sqrt(diag(vcovHC()))` from the `sandwich` package, or use `lm_robust` from Graeme's `estimatr` package (default is HC2). The default in STATA is HC1, and the default in the `sandwich` package is HC3. 

One concept you don't want to get confused with is the **clustered standard errors**. Check this [paper](https://arxiv.org/abs/1710.02926) for a discussion on when to use clustered standard errors. 


### Calculate OLS regression manually in R
```{r}
set.seed(2025)
n <- 100

x1 <- rnorm(n)
x2 <- rnorm(n)
x3 <- rnorm(n)

X <- cbind(rep(1, 100), x1, x2, x3) # why do we include rep(1, 100)? 
y <- x1 + x2 + x3 + rnorm(n)

solve(t(X) %*% X) %*% t(X) %*% y

lm(y ~ x1 + x2 + x3)$coef # the results are the same!
```


## Problem Set Hints
The purpose of Problem Set 0 is a refresher on the basic concepts of probability theory, linear algebra, and OLS regression, so that you are prepared for the material in 200C.

- Problem Set 0, Problem 1 <span style="color: darkred;">hint</span>: similar to the Example 5.7 in Math Prefresher.
- Problem Set 0, Problem 2 <span style="color: darkred;">hint</span>: refer to the properties of expected values and variance. 
- Problem Set 0, Problem 3 <span style="color: darkred;">hint</span>: refer to section 3 of **Basic Linear Algebra Review**. Also see **OLS in Matrix Form**. 
- Problem Set 0, Problem 4 <span style="color: darkred;">hint</span>: 200B (or an equivalent course) should already prepare you for terms such as bias, efficiency, consistency, WLLN, CLT, homoskedasticity, multicollinearity, etc. There are tons of materials online if you need a refresh. 
- Problem Set 0, Problem 5 <span style="color: darkred;">hint</span>: check the `Simulation` section. Run `help(distributions)` in your R console to see how to call distributions in R. 


### Acknowledgement
I develop this document based on materials from the previous TAs for PS 200C (including Doeun Kim,  Soonhong Cho, Luke Sonnet, Aaron Rudkin, Ciara Sterbenz). I thank Soonhong Cho for sharing the material on SEs.
