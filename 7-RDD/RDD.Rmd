---
title: "Pol Sci 200C Section: Regression Discontinuity Design"
author: "Haotian (Barney) Chen"
date: "May 30, 2025" 
output:
  html_document:
    math_method: mathjax
    mathjax: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, fig.align = 'center')
```

\newcommand{\y}{\mathbf{y}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\independent}{\perp\!\!\!\perp}


## Regression Discontinuity Design
***

- Sharp RDD: find a fixed cutoff point $c$ of a running (forcing) variable $X$ which determines the treatment assignment such that $D_i = \mathbf{1}_{\{X_i \geq c\}}$. If $D_i$ is only probabilistically (not **completely**) determined, it's called "fuzzy" RDD. 
 
- Example 1: close elections as RD design
     + Running variable ($X_i$) is the Democratic vote margin for district $i$.
     + Treatment ($D_i$) is winning the current election at $t$, so vote margin $c = 0$ (e.g., being an "incumbent" party/candidate for the next election at $t+1$).
     + Outcome ($Y_i$). For example, if we're interested in incumbency effect, then outcome is winning probability or vote share in the next election at $t+1$.
     + The "discontinuity" at the cutoff point is our estimate.
     + Read Chad's [paper](https://resolve.cambridge.org/core/journals/political-analysis/article/seeing-like-a-district-understanding-what-closeelection-designs-for-leader-characteristics-can-and-cannot-tell-us/77782269233D983FD37019E7A82189CB) on politician characteristic regression discontinuity.  

- Example 2: How Did Absentee Voting Affect the 2020 U.S. Election? Yoder et al. (2021) 
     + Setting: 65-year-olds can vote-by-mail without an excuse, while 64-year-olds must provide an excuse to vote absentee.
     + Exercise: What is the running variable? What is the treatment? What is the outcome variable? 
     + $Y_{iat} = \beta_t (\text{Age}=65)_{iat} + \delta_t + \epsilon_{iat}$, where $\beta_t$ is the gap between 65- and 64-year-olds, and $\delta_t$ is election fixed effects. 
 
<p align="center">
  <img width="600" height="380" src="yoder2021.png" caption="Yoder et al. (2021)">
</p>  

 <br>
 
### Identification

- Estimand: $\E[Y_{1i} - Y_{0i} \mid X_i=c]$
 
- Assumption: $\E[Y_{1i} \mid X_i=x]$ and $\E[Y_{0i} \mid X_i=x]$ are **<span style="color:red;">continuous</span>** in $X_i$ around the threshold $c$. It requires that the only change occurring at the cutoff is the shift in the treatment status.
     + E.g., $Y_{1i}$: the Democratic vote share in the next election $t+1$, if a Democratic (Republican) candidate won the election in the current period $t$, $D_i=1$ ($D_i=0$). Democratic victory is the treatment, GOP winning is the control.

     + The continuity assumption can be written as $$\E[Y_{1i} \mid X_i=c] = \lim_{x \downarrow c} \E[Y_{1i} \mid X_i=x]$$ and $$\E[Y_{0i} \mid X_i=c] = \lim_{x \uparrow c} \E[Y_{0i} \mid X_i=x],$$ where the limit is taken from above/below the threshold $c$. It can also be written that the potential outcomes are the same for $c - \epsilon < X_i < c+\epsilon$ as $\epsilon$ goes to 0:$$\E[Y_{1i} \mid X_i=c] = \lim_{\epsilon \downarrow 0} \E[Y_{1i} \mid X_i = c + \epsilon, D_i=1]$$ and $$\E[Y_{0i} \mid X_i=c] = \lim_{\epsilon \uparrow 0} \E[Y_{0i} \mid X_i =c + \epsilon, D_i=0]$$

     + It implies that there is no **discontinuous jump** in the conditional expectation function of each potential outcome. Or, we're assuming that the conditional expectation of potential outcomes at $c$ can be approximated well from one side rather than from both sides.
     
- This "*Continuity-Based* Approach" to RD design is different from **local randomization** framework for RD design:
     + Treatment assignment is as-if random near the cutoff: $\{ Y_{1i}, Y_{0i} \independent D_i|X_i \approx c\}$.
     + Local randomization assumption near the threshold is stricter than the required continuity assumption. Within the window, the average potential outcomes must be **identical** below and above the threshold under local randomization, whereas continuity assumption does not require it.
     
 
- Formally, LATE at $c$ can be identified under the continuity assumption:
 \begin{align*}
 \tau_{RDD} = \E[Y_{1i} - Y_{0i} \mid X_i=c] &= \lim_{x \downarrow c} \E[Y_{1i} \mid X_i=x] - \lim_{x \uparrow c} \E[Y_{0i} \mid X_i=x] \\
 &= \underbrace{\lim_{x \downarrow c} \E[Y_{i} \mid X_i=x]}_{\text{average just above }c} - \underbrace{\lim_{x \uparrow c} \E[Y_{i} \mid X_i=x]}_{\text{average just below }c}
 \end{align*}

- High internal validity, poor external validity. 
 
 <br>

### Estimation

Parametric models:

- Linear with same slope: $\E[Y_i | X_i, D_i] = \alpha + \beta_1 X_i + \beta_2 D_i$
   + regress $Y_i$ on $D_i$ and $\tilde X_i$.
- Linear with different slope: $\E[Y_i | X_i, D_i] = \alpha + \beta_1 X_i + \beta_2 D_i + \beta_3 X_i \cdot D_i$
   + regress $Y_i$ on $D_i$, $\tilde X_i$, and interaction term $\tilde X_i \cdot D_i$.
- Non-linear: regress $Y_i$ on $D_i$, higher-order $\tilde X_i$, and their interactions. 

Non-parametric (local) models:

- In practice, we now rely mainly on more local, flexible models such as lowess or kernel smoothers.

- For example, for kernel estimation, we calculate (weighted) moving averages to estimate $\E[Y \mid X]$, with weighting (in certain way) each unit at each value of $X=x$.

  + We're thinking of weighted means like $\widehat{\E[Y \mid X]} = \sum_{i=1}^n W_i(x) y_i$ subject to $\sum_{i} W_i(x)=1$. We typically use "kernel" function for weighting as $W_i(x) = \frac{K\left( \frac{x-x_i}{h} \right)}{\sum_{i=1}^n K\left( \frac{x-x_i}{h} \right)}$, with $h$ is called "bandwidth". 
  
  + Kernel function determines weighting scheme.
    1. Uniform kernel: each unit is given equal weight.
    2. Triangular kernel: farther units are given smaller weights, using linear distance.
    3. Gaussian kernel: farther units are given less weight, according to normal density function.
    
  + We apply this idea to regression setting: $$(\hat{\alpha}_{+}, \hat{\beta}_{+}) = \underset{\alpha, \beta}{\text{argmin}} \sum_{i=1}^n \mathbf{1}\{ X_i > c\}\{Y_i - \alpha - (X_i-c)\beta\}^2 \cdot K \left( \frac{X_i-c}{h} \right),$$ and $$(\hat{\alpha}_{-}, \hat{\beta}_{-}) = \underset{\alpha, \beta}{\text{argmin}} \sum_{i=1}^n \mathbf{1}\{ X_i < c\}\{Y_i - \alpha - (X_i-c)\beta\}^2 \cdot K \left( \frac{X_i-c}{h} \right),$$ then $\hat{\tau}^{RD} = \hat{\alpha}_{+} - \hat{\alpha}_{-}.$
        
- In practice, use `rdrobust` or `RDD` for automatic bandwidth selection for local polynomial models.

 <br>

How it works:

```{r, echo=FALSE, out.width='70%'}
knitr::include_graphics("rdd.gif")
```
(Credit: Apoorva Lal and Nick Huntington-Klein)

<br>

### RDD Coding Example
 
- Incumbent-party advantage: what is the effect of the Democratic party winning a Senate seat on its vote share in the following election for that seat? See Cattaneo, Frandsen, and Titiunik ([2015](https://cattaneo.princeton.edu/papers/Cattaneo-Frandsen-Titiunik_2015_JCI.pdf)) for details.


```{r}
library(haven)
library(tidyverse)
library(rdrobust) # RDD package
library(estimatr)
library(stargazer)
library(knitr)

#incumbent advantage in US Senate election
data_rd <- read_dta("senate.dta")
data_rd <- as_tibble(data_rd) %>%
  #forcing (running) variable: `margin` / outcome variable: 'vote' -- vote share
  mutate(margin = demmv, vote = demvoteshfor2) %>% 
  #treatment (`incumbent`) as a deterministic function of running variable ("Sharp RDD")
  mutate(incumbent = margin>=0) #cutoff is when margin=0, i.e., X_i=c=0

#Sharp RDD
#Pr(D=1) changes exactly from zero to one at the cutoff!
ggplot(data_rd, aes(x=margin, y=incumbent, color=as.factor(incumbent))) +
  geom_point(alpha = 0.5, size = 0.3) +
  labs(title = 'Incumbency as a function of past vote margin', 
       y="Prob of Receiving Treatment (Incumbency)", x="Vote Margin") + 
  theme_bw()
```

#### Quick visual inspection
```{r}
#raw comparison of means
rdplot(data_rd$vote, data_rd$margin,
       c = 0, #c: cutoff (default is 0)
       p = 0, #p: order of polynomial, so 0 is for mean
       nbins = c(2500, 500), #number of bins: usually you don't have to specify it
       col.lines = "red", col.dots = "gray", 
       x.label = "Past Election Margin", y.label = "Current Vote share", title = "", y.lim = c(0,100))

#local comparison of means
rdplot(data_rd$vote[abs(data_rd$margin) <= 10], data_rd$margin[abs(data_rd$margin) <= 10],
       c = 0, 
       p = 2, #polynomial of order 2 (quadratic)
       nbins = c(2500, 500), 
       col.lines = "red", col.dots = "gray", 
       x.label = "Past Election Margin", y.label = "Current Vote share", title = "", y.lim = c(0,100))
```


#### Point Estimation -- manually 
Let's start with a simple regression framework with same slope / different slope models.

```{r}
## Estimating treatment effects
# 1. 
#simplest: linear, same slope
lm(vote ~ incumbent + margin, data=data_rd)

#linear, but different slope: use interaction term!
lm(vote ~ incumbent*margin, data=data_rd)

#with second-order terms (w/ another syntax)
lm(vote ~ incumbent + margin + I(margin^2) + incumbent:margin + incumbent:I(margin^2), data=data_rd)
```

Next, let's turn to "local" estimation. That is, we're choosing a small window around the threshold $c$, then do the same analyses in the chosen window (technically, it's the same as the local linear RD estimate with a uniform kernel). I chose an arbitrary bandwidth of 10 -- the window around the cutoff.   

```{r}
#(local) linear regression
summary(lm(vote ~ incumbent + margin, data=data_rd %>% filter(abs(margin) < 10)))$coefficients
summary(lm(vote ~ incumbent*margin, data=data_rd %>% filter(abs(margin) < 10)))$coefficients

#(local) polynomial fit
summary(lm(vote ~ incumbent + margin + I(margin^2) + incumbent:margin + incumbent:I(margin^2), 
           data=data_rd %>% filter(abs(margin) < 10)))$coefficients
```

#### Point Estimation -- rdrobust
In practice, use `rdrobust` function in the `rdrobust` package, which gives us RD estimates with optimal bandwidth selection as well as valid standard errors and confidence intervals.

```{r}
# with uniform kernel: do we get the same estimate?
summary(rdrobust(data_rd$vote, data_rd$margin, kernel = 'uniform', p = 1, h = 10)) #h is bandwidth

# with triangular kernel
summary(rdrobust(data_rd$vote, data_rd$margin, kernel = 'triangular', p = 1, h = 10))

# let rdrobust select an MSE-optimal bandwidth for the local linear estimator
summary(rdrobust(data_rd$vote, data_rd$margin, kernel = 'triangular', p = 1))
```

The "BW type" shows the choice of bandwidth-selection method. `mserd` is an MSE-optimal version. Let's look at the plot.

```{r}
#MSE-optimally chosen bandwidth
bandwidth <- rdrobust(data_rd$vote, data_rd$margin, kernel = 'triangular', p = 1)$bws[1, 1]
rdplot(data_rd$vote[abs(data_rd$margin) <= bandwidth], data_rd$margin[abs(data_rd$margin) <= bandwidth],
       c = 0, p = 1, col.lines = "red", col.dots = "gray", 
       x.label = "Past Election Margin", y.label = "Current Vote share", 
       title = "Local Polynomial Estimate with Triangular Kernel and Bandwidth=17.75", y.lim = c(0,100))
```

We can include covariates using `covs` option. Note that this covariate-adjustment will also change the optimal bandwidth.

```{r}
covariates <- data_rd %>% select(presdemvoteshlag1, demvoteshlag1, demvoteshlag2, dmidterm)
summary(rdrobust(data_rd$vote, covs = covariates, data_rd$margin, 
                 kernel = 'triangular', p = 1, bwselect = 'mserd'))
```

### Validation and Falsification of RD Design

- Threat to validity: what are the scenarios under which the continuity assumption is violated?

- **<span style="color:red;">Sorting </span>**: if the cutoff is known to the units, then we have concerns about the possibility of units actively manipulating the forcing variable. For example, if (the parties of) barely-losers can precisely predict the election outcome, they will try to manipulate the final vote share, which will push them up above the threshold and make them incumbents. Such sorting, if possible, leads to a discontinuous jump at the cutoff point in the CEF of potential outcomes (vote share in next election).
 
- Testing the validity of the RD Design: we have some options to provide empirical evidence supporting the validity of the design.
    1. **Density test of sorting** (McCrary test): the continuity assumption may be violated if some candidates can precisely manipulate their margin of victory (sorting). The McCrary test checks this possibility, of the manipulation of the running variable checks for a discontinuity in the density at the threshold. Too different density *suggests* (not "prove") that some units could have sorted to one side or the other.
    
    2. **Covariates as placebo outcomes**: good placebo variables are (1) expected not to have any effect on the actual outcome and (2) closely related to outcome of interest. The idea is that if placebo outcomes are not continuous at the cutoff, the continuity of the potential outcome functions is unlikely to hold (i.e., continuity assumption is violated), and thus the validity of the design is called into question.
    
    3. **Placebo cutoffs**: we can examine the effects at fake (placebo) cutoff points. The presence of discontinuities away from the cutoff can be a bad news.

 <br>
 
First, McCrary test.

```{r}
library(rddensity)
library(lpdensity)
library(rdd)

## 1. Density test of sorting
#looking at estimated density: using `rddensity` and `lpdensity`
bw_left <- as.numeric(rddensity(data_rd$margin)$h[1])
bw_right <- as.numeric(rddensity(data_rd$margin)$h[2])
est1 <- lpdensity(data = data_rd$margin[data_rd$margin < 0 & data_rd$margin >= -bw_left], 
                  grid = seq(-bw_left, 0, 0.1), bwselect = "mse-dpi",
                  scale = sum(data_rd$margin < 0 & data_rd$margin >= -bw_left) / length(data_rd$margin))
est2 <- lpdensity(data = data_rd$margin[data_rd$margin >= 0 & data_rd$margin <= bw_right], 
                  grid = seq(0, bw_right, 0.1), bwselect = "mse-dpi",
                  scale = sum(data_rd$margin >= 0 & data_rd$margin <= bw_right) / length(data_rd$margin))

lpdensity.plot(est1, est2, CItype="line", CIshade = 0.2, lcol = c(1, 2), 
               CIcol = c(1, 2), legendGroups = c("Control", "Treatment")) +
  geom_vline(xintercept = 0, color = "black") +
  labs(x = "Past Democratic Margin", y = "Estimated Density") +
  theme_bw()
       
#An easier option: using `DCdensity` from rdd package
rdd::DCdensity(data_rd$margin, cutpoint = 0)
```

We fail to reject the null that the density is the same on either side of the cutoff -- no evidence of self-sorting. 

Second, covariates as placebo outcomes. 

```{r}
## 2. Pre-treatment covariate as outcome
#conduct a placebo test with the pre-treatment covariates instead of the outcome.
#(1) state-level Democratic percentage of the vote in the past presidential election
bandwidth <- rdrobust(data_rd$presdemvoteshlag1, data_rd$margin)$bws[1,1]
rdplot(data_rd$presdemvoteshlag1[abs(data_rd$margin)<=bandwidth], 
       data_rd$margin[abs(data_rd$margin)<=bandwidth],
       p=3, kernel='triangular', x.lim=c(-ceiling(bandwidth), ceiling(bandwidth)), 
       col.lines = "red", col.dots = "gray",
       x.label = "Democratic margin", title="",
       y.label = "Democratic vote share in past presidential election")
```
The plot does not suggest clear discontinuities at the cutoff. But a statistical test is required, which can be done by running `rdrobust` using each covariate as the (placebo) outcome.

```{r}
summary(rdrobust(data_rd$presdemvoteshlag1, data_rd$margin))
```


```{r}
#(2) lagged outcome is always good: future cannot affect past
#Democratic percentage of the vote in the t-2 Senate election
bandwidth <- rdrobust(data_rd$demvoteshlag1, data_rd$margin)$bws[1,1]
rdplot(data_rd$demvoteshlag1[abs(data_rd$margin)<=bandwidth], 
       data_rd$margin[abs(data_rd$margin)<=bandwidth],
       p=3, kernel='triangular', x.lim=c(-ceiling(bandwidth),ceiling(bandwidth)),
       col.lines = "red", col.dots = "gray", 
       x.label = "Democratic margin", title="", y.label = "Past vote share")

summary(rdrobust(data_rd$demvoteshlag1, data_rd$margin))
```

Finally, placebo cutoffs.

(A technical point: we need to restrict the data in some ways. To avoid contamination due to real treatment effects, for placebo cutoffs above the actual cutoff (0) only treated units are included, and for artificial cutoffs below 0 only control units are used. This restriction ensures that the analysis of each placebo cutoff uses only observations with the same treatment status, which creates the placebo treatment effects of zero)

```{r}
#a placebo cutoff c=30%
rdplot(data_rd$vote[data_rd$margin >= 0], data_rd$margin[data_rd$margin >= 0], c=30,
       col.lines = "red", col.dots = "gray")
DCdensity(data_rd$margin, cutpoint = 30)
summary(rdrobust(data_rd$vote[data_rd$margin >= 0], data_rd$margin[data_rd$margin >= 0], c=30))

#a placebo cutoff c=-30%
rdplot(data_rd$vote[data_rd$margin <= 0], data_rd$margin[data_rd$margin <= 0], c=-30,
       col.lines = "red", col.dots = "gray")
DCdensity(data_rd$margin, cutpoint = -30)
summary(rdrobust(data_rd$vote[data_rd$margin <= 0], data_rd$margin[data_rd$margin <= 0], c=-30))
```

### Acknowledgement
I develop this document referencing to materials from Soonhong Cho, the previous TA for PS 200C.
